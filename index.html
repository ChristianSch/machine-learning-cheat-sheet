<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="HandheldFriendly" content="True">
		<meta name="MobileOptimized" content="320">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">

		<link rel="stylesheet" href="css/main.css" type="text/css" media="all">

		<!-- typeplate -->
		<link rel="stylesheet" href="css/typeplate.css" type="text/css" media="all">

		<!-- MathJax -->
		<script type="text/x-mathjax-config">
			MathJax.Hub.Config({
				extensions: ["tex2jax.js"],
				jax: ["input/TeX", "output/HTML-CSS"],
				tex2jax: {
					inlineMath: [ ['$','$'], ["\\(","\\)"] ],
					displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
					processEscapes: true
				},
				"HTML-CSS": { availableFonts: ["TeX"] }
			});
		</script>
		<script type="text/javascript" src="js/MathJax/MathJax.js"></script>
	</head>

	<body>
		<div id="main-wrap">
			<h1>Machine Learning @ Coursera</h1>
			<h2 class="sub-h2">A cheat sheet</h2>

			<p>This cheatsheet wants to provide an overview of the concepts and the used formulas and definitions of the <em>»Machine Learning«</em> <a href="https://class.coursera.org/ml-008" title="Coursera Online Course: Machine Learning">online course at coursera</a>.</p>

			<div id="last-change">
				<p>Last changed: <time datetime="2015-01-27">January 27th, 2015</time></p>
			</div>

			<h2 class="week-indicator">Week 1</h2>

			<h3>Introduction</h3>

			<h4 class="definition">Machine Learning</h4>
			<blockquote>
				»Well-posed Learning Problem: A computer program is said to <em>learn</em>
				from experience E with respect to some task T and some performance measure E,
				if its performance on T, as measured by P, improves with experience E.«
				<br>
				— <cite>Tom Mitchell (1998)</cite>
				<sup><a href="#footnote1">[1]</a></sup>
			</blockquote>

			<h4 class="definition">Supervised Learning</h4>
			TODO

			<h4 class="definition">Unsupervised Learning</h4>
			TODO

			<h4 class="definition">Regression Problem</h4>
			TODO

			<h4 class="definition">Classification Problem</h4>
			TODO

			<h3>Linear Regression with One Variable</h3>
			<h4>Model Representation</h4>
			<p>
				Notations:

				<ul>
					<li>$m$: Number of training examples</li>
					<li>$x$'s: <em>input</em> variable/features</li>
					<li>$y$'s: <em>output</em> variables/<em>target</em variable></li>
					<li>$(x,y)$: one training example</li>
					<li>$(x^{(i)},y^{(i)})$: $i$<sup>th</sup> training example</li>
				</ul>
			</p>

			<h5 class="definition">Hypothesis</h5>
			<p>The hypothesis function maps <em>x's</em> to <em>y's</em> ($h: x \mapsto y$). It can be represented by:</p>

			$$h_{\theta} = \theta_{0} + \theta_{1}x$$

			<p>The shorthand for the hypothesis is $h(x)$. $\theta_{i}$'s are called the parameters of the hypothesis.
			In this case the hypothesis is called <em>»Univariate Linear Regression«</em>.</p>

			<h4>Cost Function</h4>
			<h5 class="definition">Squared Error Cost Function</h5>
			$$J(\theta_0, \theta_1) = \frac{1}{2m} \sum^{m}_{i=1}\left(h_{\theta}(x^{(i)}) - y^{(i)}\right)^2$$

			<h4>The Gradient Descent Algorithm</h4>
			<p>The goal is to choose the hypothises' parameters in a manner that the the output of the cost function $J(\theta_0, \theta_1)$ is minimal.</p>

			<p><strong>Outline:</strong>
				<ul>
					<li>Start with some $\theta_0, \theta_1$</li>
					<li>Keep changing $\theta_0, \theta_1$ to reduce $J(\theta_0, \theta_1)$ until we hopefully end up at a minimum</li>
				</ul>
			</p>

			<h5 class="definition">Gradient Descent Algorithm</h5>
			<p>Note that the values of $\theta_0$ and $\theta_1$ are updated simultaneously. $\alpha$ is called the <em>learning rate.</em></p>
			$$\begin{aligned}
			\text{repeat} & \text{ until convergence} \; \{\\
			& \theta_j := \theta_j - \alpha \frac{\delta}{\delta\theta_j} J(\theta_0, \theta_1) \;\; \text{(for j=0 and j=1)}\\
			\} \phantom{15pt} &
			\end{aligned}$$

			<p>The correct implementation of the simultaneous update looks like this:</p>
			$$\begin{aligned}
			& temp0 := \theta_0 - \alpha \frac{\delta}{\delta\theta_j} J(\theta_0, \theta_1)\\
			& temp1 := \theta_1 - \alpha \frac{\delta}{\delta\theta_j} J(\theta_0, \theta_1)\\
			& \theta_0 := temp0 \\
			& \theta_1 := temp1
			\end{aligned}$$ 

			<h5>Partial derivitive of the cost function</h5>
			$$\begin{aligned}
			\text{repeat} & \text{ until convergence} \; \{\\
			& \theta_0 := \theta_0 - \alpha \frac1{1}{m}\sum^{m}_{i=0}\left(h_\theta(x^{(i)}) - y^{(i)}\right)\\
			& \theta_1 := \theta_1 - \alpha \frac1{1}{m}\sum^{m}_{i=0}\left(h_\theta(x^{(i)}) - y^{(i)}\right) \cdot x^{(i)}\\
			\} \phantom{15pt} &
			\end{aligned}$$


			<p><strong>Note:</strong> The cost function $J(\theta_0, \theta_1)$ is <em>convex</em> and therefore has only one optimum overall.</p>

			<h5>Batch Gradient Descent</h5>
			<p>If the gradient descent uses all $m$ training examples in each iteration it is also called <em>Batch Gradient Descent Algorithm</em>.</p>

			<h2 class="week-indicator">Week 2</h2>
			<h3>Linear Regression with Multiple Variables</h3>
			<h3>Octave Tutorial</h3>

			<hr>

			<h2>Appendix</h2>
			<h3>Footnotes</h3>
			<ol>
				<li id="footnote1">Taken from the Machine Learning Video Lecture: »What is Machine Learning?«, Minute <code>01:49</code> at <a href="https://class.coursera.org/ml-008/lecture/2" title="Video Lecture on Coursera">Coursera</a></li>
			</ol>
		</div>
	</body>
</html>
