<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="HandheldFriendly" content="True">
        <meta name="MobileOptimized" content="320">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">

        <title>Machine Learning @ Stanford - A Cheat Sheet</title>

        <link rel="stylesheet" href="css/main.css" type="text/css" media="all">

        <!-- typeplate -->
        <link rel="stylesheet" href="css/typeplate.css" type="text/css" media="all">

        <!-- MathJax -->
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                extensions: ["tex2jax.js"],
                jax: ["input/TeX", "output/HTML-CSS"],
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true
                },
                "HTML-CSS": { availableFonts: ["TeX"] }
            });
        </script>
        <script type="text/javascript" src="js/MathJax/MathJax.js"></script>

        <!-- Highlight.js -->
        <link rel="stylesheet" href="js/Highlight.js/styles/default.css">
        <script src="js/Highlight.js/highlight.pack.js" type="text/javascript"></script>
        <script>hljs.initHighlightingOnLoad();</script>
    </head>

    <body id="top">
        <div id="main-wrap">
            <h1>Machine Learning @ Coursera</h1>
            <h2 class="sub-h2">A cheat sheet</h2>

            <p>This cheatsheet wants to provide an overview of the concepts and the used formulas and definitions of the <em>»Machine Learning«</em> <a href="https://class.coursera.org/ml-008" title="Coursera Online Course: Machine Learning">online course at coursera</a>.</p>

            <div id="last-change">
                <p>Last changed: <time datetime="2015-02-17">February 17th, 2015</time></p>
            </div>

            <p><strong>Please note:</strong> I changed the notation very slighty. I'll denote vectors with a little arrow on the top. Example: $\vec\theta$</p>

            <p>The <a href="octave.html" title="octave tutorial">octave tutorial</a> that was part of the seond week is available as a script <a href="octave.html" title="octave tutorial">here</a>.</p>

            <h2 class="week-indicator" id="week-1">Week 1<a class="anchor-link" href="#week-1"></a></h2>

            <h3 id="intro">Introduction<a class="anchor-link" href="#intro"></a></h3>

            <h4 class="definition" id="def-ml">Machine Learning<a class="anchor-link" href="#def-ml"></a></h4>
            <blockquote>
                »Well-posed Learning Problem: A computer program is said to <em>learn</em>
                from experience E with respect to some task T and some performance measure E,
                if its performance on T, as measured by P, improves with experience E.«
                <br>
                — <cite>Tom Mitchell (1998)</cite>
                <sup><a href="#footnote1">[1]</a></sup>
            </blockquote>

            <h4 class="definition" id="def-sl">Supervised Learning<a class="anchor-link" href="#def-sl"></a></h4>
            <p>Supervised learning is the task of learning to predict the answers
                for unseen examples where the right answers for some
                examples are given to learn from.</p>

            <h4 class="definition" id="def-ul">Unsupervised Learning<a class="anchor-link" href="#def-ul"></a></h4>
            <p>Unsupervised learning is learning in the absence of right answers
            to learn from. The algorithm discovers structure in the data on its
            own.</p>

            <h4 class="definition" id="def-reg-prob">Regression Problem<a class="anchor-link" href="#def-reg-prob"></a></h4>
            <p>Regression problems need estimators to predict continuous output,
                such as house prices.</p>

            <h4 class="definition" id="def-class-prob">Classification Problem<a class="anchor-link" href="#def-class-prob"></a></h4>
            <p>Classification problems need estimators to predict categorical (discrete)
                valued output. (I.e. $0$ or $1$, whether or not a patient has cancer
                or not.)</p>

            <h3 id="univariate-lin-reg">Linear Regression with One Variable<a class="anchor-link" href="#univariate-lin-reg"></a></h3>
            <h4 id="model-rep">Model Representation<a class="anchor-link" href="#model-rep"></a></h4>
            <p>
                <strong>Notations:</strong>
                <ul>
                    <li>$m$: number of training examples</li>
                    <li>$x$'s: <em>input</em> variable/features</li>
                    <li>$y$'s: <em>output</em> variables/<em>target</em variable></li>
                    <li>$(x,y)$: one training example</li>
                    <li>$(x^{(i)},y^{(i)})$: $i$<sup>th</sup> training example</li>
                </ul>
            </p>

            <h5 class="definition" id="univariate-hypothesis">Univariate Hypothesis<a class="anchor-link" href="#univariate-hypothesis"></a></h5>
            <p>The hypothesis function maps <em>x's</em> to <em>y's</em> ($h: x \mapsto y$). It can be represented by:</p>

            $$h_{\theta} = \theta_{0} + \theta_{1}x$$

            <p>The shorthand for the hypothesis is $h(x)$. $\theta_{i}$'s are called the parameters of the hypothesis.
            In this case the hypothesis is called <em>»Univariate Linear Regression«</em>.</p>

            <h4 id="cost-function">Cost Function<a class="anchor-link" href="#cost-function"></a></h4>
            <p>Idea: Choose $\theta_0, \theta_1$ so that $h_\theta(x)$ is close to $y$
            for our training examples $(x,y)$. The 3D-surface plot below visualizes
            the cost function for the two parameters.</p>

            <img src="img/cost_function_surface.png"
                 alt="Cost Function 3D Surface Plot">

            <h5 class="definition" id="def-secf">Squared Error Cost Function<a class="anchor-link" href="#def-secf"></a></h5>
            $$J(\theta_0, \theta_1) = \frac{1}{2m} \sum^{m}_{i=1}\left(h_{\theta}(x^{(i)}) - y^{(i)}\right)^2$$

            <h4 class="def" id="grad-dec">The Gradient Descent Algorithm<a class="anchor-link" href="#grad-dec"></a></h4>
            <p>The goal is to choose the hypothises' parameters in a manner that the the output of the cost function $J(\theta_0, \theta_1)$ is minimal.</p>

            <p><strong>Outline:</strong>
                <ul>
                    <li>Start with some $\theta_0, \theta_1$</li>
                    <li>Keep changing $\theta_0, \theta_1$ to reduce $J(\theta_0, \theta_1)$ until we hopefully end up at a minimum</li>
                </ul>
            </p>

            <h5 class="definition" id="def-grad-dec">Gradient Descent Algorithm<a class="anchor-link" href="#def-grad-dec"></a></h5>
            <p>Note that the values of $\theta_0$ and $\theta_1$ are updated simultaneously. $\alpha$ is called the <em>learning rate.</em></p>
            $$\begin{aligned}
            \text{repeat} & \text{ until convergence} \; \{\\
            & \theta_j := \theta_j - \alpha \frac{\delta}{\delta\theta_j} J(\theta_0, \theta_1) \;\; \text{(for j=0 and j=1)}\\
            \} \phantom{15pt} &
            \end{aligned}$$

            <p>The correct implementation of the simultaneous update looks like this:</p>
            $$\begin{aligned}
            & temp0 := \theta_0 - \alpha \frac{\delta}{\delta\theta_j} J(\theta_0, \theta_1)\\
            & temp1 := \theta_1 - \alpha \frac{\delta}{\delta\theta_j} J(\theta_0, \theta_1)\\
            & \theta_0 := temp0 \\
            & \theta_1 := temp1
            \end{aligned}$$

            <h5 id="part-der-cf">Partial derivitive of the cost function<a class="anchor-link" href="#part-der-cf"></a></h5>
            $$\begin{aligned}
            \text{repeat} & \text{ until convergence} \; \{\\
            & \theta_0 := \theta_0 - \alpha \frac{1}{m} \sum^{m}_{i=0}\left(h_\theta(x^{(i)}) - y^{(i)}\right)\\
            & \theta_1 := \theta_1 - \alpha \frac{1}{m} \sum^{m}_{i=0}\left(h_\theta(x^{(i)}) - y^{(i)}\right) \cdot x^{(i)}\\
            \} \phantom{15pt} &
            \end{aligned}$$


            <p><strong>Note:</strong> The cost function $J(\theta_0, \theta_1)$ is <em>convex</em> and therefore has only one optimum overall.</p>

            <h5 class="definition" id="def-batch-grad-dec">Batch Gradient Descent<a class="anchor-link" href="#def-batch-grad-dec"></a></h5>
            <p>If the gradient descent uses all $m$ training examples in each iteration it is also called <em>Batch Gradient Descent Algorithm</em>.</p>

            <h2 class="week-indicator" id="week-2">Week 2<a class="anchor-link" href="#week-2"></a></h2>
            <h3 id="multivariate-lin-reg">Linear Regression with Multiple Variables<a class="anchor-link" href="#multivariate-lin-reg"></a></h3>
            <h4 id="multiple-features">Multiple Features<a class="anchor-link" href="#multiple-features"></a></h4>
            <p>
                <strong>Notations:</strong>
                <ul>
                    <li>$n$: number of features</li>
                    <li>$\vec x^{(i)}$: input features of <em>i<sup>th<sup></em> training example</li>
                    <li>$x^{(i)}_j$: value of feature $j$ in <em>i<sup>th<sup></em> training example</li>
                </ul>
            </p>

            <h5 class="definition" id="multivariate-hypothesis">Multivariate Hypothesis<a class="anchor-link" href="#multivariate-hypothesis"></a></h5>
            $$h_\theta = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_n x_n$$

            <p>For convenience of notation we'll define $x_0 = 1$ ($x_0^{(i)}$). Thus a feature vector looks like:</p>

            $$ \vec x = \left[\begin{array}{c} x_0\\ x_1 \\ \vdots \\ x_n \end{array}\right] \in \mathbb{R}^{n+1}$$

            <p>And the parameters can be written as:</p>

            $$ \vec \theta = \left[\begin{array}{c} \theta_0\\ \theta_1 \\ \vdots \\ \theta_n \end{array}\right] \in \mathbb{R}^{n+1}$$

            <p>The hypothesis can now be written as:</p>

            $$\begin{align}
            h_\theta & = \theta_0 x_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_n x_n\\
            & = \left[ \theta_0 \theta_1 \cdots \theta_n \right] \left[\begin{array}{c} \theta_0\\ \theta_1 \\ \vdots \\ \theta_n \end{array}\right] \\
            & = \boxed{\vec\theta^{T}\vec x}\\
            \end{align}$$

            <p>This hypothesis is called <em>»Multivariate Linear Regression«</em>.</p>

            <h4 id="multivar-grad-desc">Gradient Descent for Multiple Variables<a class="anchor-link" href="#multivar-grad-desc"></a></h4>
            <p>The cost function can now be written as:</p>

            $$\begin{aligned}
            J(\theta_0, \theta_1, ..., \theta_n) & = \frac{1}{2m}\sum^{m}_{i=1} \left(h_\theta(x^{(i)}) - y^{(i)}\right)^2\\
            & = \boxed{J(\vec\theta)}
            \end{aligned}$$

            <p>The gradient descent algorithm for this cost function looks like this:</p>

            $$\begin{aligned}
            \text{repeat} & \{\\
            & \theta_j := \theta_j - \alpha \frac{\delta}{\delta \theta_j} \; \; J(\theta_0, \theta_1, ... , \theta_n)\\
            \} \phantom{15pt} &
            \end{aligned}$$

            <p>Which can also be written like this:</p>
            $$\begin{aligned}
            \text{repeat} & \{\\
            & \theta_j := \theta_j - \alpha \frac{1}{m} \sum^{m}_{i=1}\left( h_\theta(\vec x^{(i)}) -y^{(i)} \right) x_j^{(i)}) \\
            \} \phantom{15pt} &
            \end{aligned}$$

            <p>Again, the parameters $\theta_j \; \forall j= 0, ..., n$ have to be updated simultaneously.</p>

            <h5 id="feature-scaling">Feature Scaling<a class="anchor-link" href="#feature-scaling"></a></h5>
            <p>Get every feature into approximately a $-1 \leq x_i \leq 1$ range to
            optimize the path finding of the gradient descent algorithm.</p>

            <h6 id="mean-norm">Mean Normalization<a class="anchor-link" href="#mean-norm"></a></h6>
            <p>Replace $x_i$ with $x_i - \mu_i$ to make features have approximately zero mean
            (does not apply to $x_0 = 1$).</p>

            $$x_1 \leftarrow \frac{x_1 - \mu_1}{s_1}$$

            <p>where $\mu_1$ is the average value of x in the training set and $s_1$ is
            the range $max - min$ (or standard deviation).</p>

            <h5 id="learning-rate">Learning Rate $\alpha$<a class="anchor-link" href="#learning-rate"></a></h5>
            <p>General rule: $J(\vec\theta)$ should decrease after every iteration.</p>

            <h6>Example Automatic Convergence Test</h6>
            <p>Declare convergence if $J(\vec\theta)$ decreases by less than $\epsilon = 10^{-3}$ in one iteration.</p>

            <p>Make sure gradient descent works correctly:</p>

            <ul>
                <li>If $\alpha$ is too small: slow convergence</li>
                <li>If $\alpha$ is too large: $J(\vec\theta)$ may not decrease on every iteration and may not converge</li>
            </ul>

            <p>For $\alpha$ try: $0.001$, $0.003$, $0.01$, $0.03$, $0.1$, $0.3$, $1$ etc.</p>


            <h4 id="poly-reg">Features and Polynomial Regression<a class="anchor-link" href="#poly-reg"></a></h4>
            <p>You can choose your model to fit to a polynomial if you want it to behave in a
            specific way. You not only can choose to multiply/divide two or more features to create
            a new feature, you can also fit your model to a more complex polynomial. If you want your
            model to behave for example to increase you could choose to use $(size)^2$, fit it to a
            cubic polynomial the same way, or use $\sqrt{size}$.</p>


            <h4 id="norm-eq">Normal Equation<a class="anchor-link" href="#norm-eq"></a></h4>
            <p>An alternative to the gradient descent algorithm is an analytical solution to
            minimize the cost function $J(\vec\theta)$.</p>

            $$\theta \in \mathbb{R}^{n+1} \;\;\; J(\theta_0, \theta_1, \cdots, \theta_m) = \frac{1}{2m}\sum^{m}_{i=1} \left( h_\theta(x^{(i)} - y^{(i)}\right)^2$$

            $$\frac{\delta}{\delta\theta_j} J(\vec\theta) = \cdots = 0 \;\;\; \text{(for every $j$)}$$

            <p>Then solve for $\theta_0,\theta_1,\cdots,\theta_n$ by setting the equation to equal zero. For $m$ examples $(\vec x^{(i)}, y^{(i)}), \cdots, (\vec x^{(m)}, y^{(m)})$ with $n$ features the input feature of the <em>i<sup>th</sup></em> training example looks like this:</p>

            $$\vec x^{(i)} = \left[ \begin{array}{c} x_0^{(i)}\\ x_1^{(i)} \\ \vdots \\ x_n^{(i)} \end{array} \right] \in \mathbb{R}^{n+1}$$

            <p>The design matrix $x$ then looks like that:</p>

            $$ X = \left[ \begin{array}{c} (x^{(1)})^T \\ (x^{(2)})^T \\ \vdots \\ (x^{(m)})^T \end{array} \right]$$

            <p>and the vector of the outputs of the training examples look like this:</p>

            $$ \vec y = \left[ \begin{array}{c} y^{(1)} \\ y^{(2)} \\ \vdots \\ y^{(m)}\end{array}\right]$$

            <p>With the design matrix the minimum $\vec \theta$ is:</p>

            $$ \vec \theta =  (X^TX)^{-1} X^T \vec y$$

            <p>Note: using this method you don't need to scale the features.</p>

            <h4 id="grad-desc-vers-norm-eq">Gradient Descent versus Normal Equation<a class="anchor-link" href="#grad-desc-vers-norm-eq"></a></h4>
            <p>For $m$ training examples and $n$ features:</p>

            <div class="two-col">
                <h5>Gradient Descent:</h5>
                <ul>
                    <li>Need to choose $\alpha$</li>
                    <li>Needs many iterations</li>
                    <li>Works well even when $n$ is large</li>
                </ul>

                <br>
                <br>

                <h5>Normal Equation</h5>
                <ul>
                    <li>No need to choose $\alpha$</li>
                    <li>Don't need to iterate</li>
                    <li>Need to compute $(X^TX)^{-1})$ (complexity $O(n^3)$)</li>
                    <li>Slow if $n$ is very large</li>
                </ul>
            </div>

            <h4 id="norm-eq-noninv">Normal Equation Noninvertibility<a class="anchor-link" href="#norm-eq-noninv"></a></h4>
            <p>Although very rarely, it can happen that $X^TX$ is non-invertible,
            for example if the matrix is singular or degenerate.</p>

            <p>Problems can be that redundant features are used (and therefore the vetors are linearly dependent) or that too many features are used (e.g. $m \leq n$). The solution for that
            would be to delete some features or use regularization (which comes later in this course).</p>

            <h2 class="week-indicator" id="week-3">Week 3<a class="anchor-link" href="#week-3"></a></h2>
            <h3 id="lin-reg-3">Linear Regression<a class="anchor-link" href="#lin-reg-3"></a></h3>
            <p>With $y \in \{0, 1\}$ the linear regression model is</p>

            $$0 \leq h_\theta(x) \leq 1$$

            <h4 id="log-reg-hypo">Hypothesis for Logistic Regression Models<a class="anchor-link" href="#log-reg-hypo"></a></h4>

            <p>The hypothesis for this model is this:</p>

            $$h_\theta(x) = g(\theta^Tx)$$

            <p>Where $g$ is called <em>sigmoid function</em> or <em>logistic function</em></p>

            $$g(z) = \frac{1}{1+e^{-z}}$$

            <p>Which gives us the following representation for the hypothesis:</p>

            $$h_\theta(x) = \frac{1}{1+e^{-\theta^Tx}}$$

            <h4 id="hypo-interpr">Interpretation of the Hypothesis Output<a class="anchor-link" href="#hypo-interpr"></a></h4>
            <p>$h_\theta(x)$ is the estimated probability that $y=1$ on input $x$:</p>

            $$h_\theta(x) = P(y = 1 | x ; \theta)$$

            <p>Which reads like <em>probability that $y=1$ given x, parameterized by $\theta$</em></p>

            $$P(y=0|x;\theta) + P(y=1|x;\theta) = 1\\
            P(y=0|x;\theta) = 1 - P(y=1|x;\theta)$$

            <h4 id="decision-boundary">Decision Boundary<a class="anchor-link" href="#decision-boundary"></a></h4>
            <p>Suppose we predict $y=1$ if $h_\theta(x) \geq 0.5$. $g(z) \geq 0.5 $ when $z \geq 0$.
            For $h_\theta(x) = g(\theta^Tx) \geq 0.5$ whenever $z = \theta^Tx \geq 0$. Suppose we
            predict $y=0$ if $h_\theta(x) &lt; 0.5$.</p>
            <p>The decision boundary ist part of the hypothesis. If we for example have a hypothesis with two features, the term $\theta_0 + \theta_1 x_1 + \theta_2 x_2$ gives the decision boundary, where this is the hypothesis:</p>

            $$h_\theta(x) = g(\theta_0 + \theta_1 x_1 + \theta_2 x_2)$$

            <h5 id="non-lin-dec-bound">Non-linear Decision Boundaries<a class="anchor-link" href="#non-lin-dec-bound"></a></h5>
            <p>As <a href="#poly-reg" title="polynomial regression">previously mentioned</a> one can add custom features to fit the hypothesis to the data. This allows us to add custom features that result in a circular decision boundary.</p>
            <p>For example:</p>

            $$\vec \theta = \left[\begin{array}{c} -1\\ 0 \\ 0 \\ 1 \\ 1 \end{array}\right]\\$$

            <p>With custom features $x_1^2$ and $x_2^2$:</p>

            $$h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_1^2 + \theta_4 x_2^2$$

            <p>We predict $y=1$ if $-1 + x^2_1 + x^2_2 \geq 0$. The polynom can have as much custom features as you wish to fit the data. Even with non-circular non-linear decision boundaries.</p>

            <h4 id="log-reg-cost">Cost Function<a class="anchor-link" href="#log-reg-cost"></a></h4>
            <p>Notation:</p>
            <ul>
                <li>Training set: $\left\{ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(m)}, y^{(m)})\right\}$</li>
                <li>$m$ examples $\;\;\;x \in \left[\begin{array}{c} x_0\\ x_1 \\ \vdots \\ x_n \end{array}\right] \; \; \; x_0 = 1, y \in \{0,1\}$</li>
                <li>$h_\theta = \frac{1}{1+e^{-\theta^Tx}}$</li>
            </ul>

            <p>The cost function for linear regression was:</p>

            $$J(\vec\theta) = \frac{1}{m} \sum^m_{i=1} \frac{1}{2}\left(h_\theta(x^{(i)}) - y^{(i)}\right)^2 = cost(h_\theta(x^{(i)}), y^{(i)})\\
            cost(h_\theta(\vec x), \vec y) = \frac{1}{2}(h_\theta(\vec x) - \vec y)^2$$

            <p>This cost function is <em>non-convex</em>. For logistic regression we want a <em>convex</em> function.</p>

            $$cost(h_\theta(\vec x), \vec y) = \left\{ \begin{array}{r} -log(h_\theta(\vec x)) \;\;\; \text{if} \;\; y = 1 \\
            -log(1 - h_\theta(\vec x)) \;\;\; \text{if} \;\; y = 0\end{array}\right.$$

            <p>If the $\text{cost} \, = 0 \; \text{if} \; y = 1, h_\theta(x) = 1$ but as $h_\theta(x) \rightarrow 0$ $Cost \rightarrow \infty$. This captures the intuition that if $h_\theta(x) = 0$ (predict $P(y=1|x;\theta)$), but $y=1$ we'll penalize the learning alrogithm by a very large cost.</p>

            <h4 id="simp-cost-grad-desc">Simplified Cost Function and Gradient Descent<a class="anchor-link" href="#simp-cost-grad-desc"></a></h4>
            <p>The cost function can be simplified to a one liner:</p>

            $$cost(h_\theta(x),x) = -y \; log(h_\theta(x)) - (1-y) log(1-h_\theta(x))$$

            <p>This works because if $y=1$, the first term will be multiplicated by $1$ and the second term will be multiplicated with $(1-y) = (1-1) = 0$. If $y=0$ the first term is multiplicated with 0 and the second term is multiplicated with $(1-0) = 1$.</p>

            <p>This gives us the complete cost function $J(\vec\theta)$:</p>

            $$\begin{align}
            J(\vec\theta) & = \frac{1}{m} \sum^m_{i=1} cost(h_\theta(x^{(i)}),y^{(i)})\\
                          & = - \frac{1}{m} \left[\sum^m_{i=1} y^{(i)} log(h_\theta(x^{(i)})) + (1-y^{(i)} ) log(1 - h_\theta( x^{(i)}))\right]
            \end{align}$$

            <p>To fit parameters $\vec\theta$ $\text{min}_\theta J(\vec\theta)$ is calculated. To make a new prediction given new x the output of $h_\theta$ has to be calculated ($P(y=1|x;\theta)$).</p>

            <h5 id="log-reg-grad-desc">Gradient Descent<a class="anchor-link" href="#log-reg-grad-desc"></a></h5>
            <p>$\text{min}_\theta J(\theta)$:</p>

            $$\begin{aligned}
            \text{repeat} &\{\\
            & \theta_j := \theta_j - \alpha \sum^m_{i=1} (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}\\
            \} \phantom{15pt} &
            \end{aligned}$$

            <h4 id="log-reg-ad-opt">Advanced Optimization<a class="anchor-link" href="#log-reg-ad-opt"></a></h4>
            <p>In general we need to compute two things for our optimization:</p>

            <ul>
                <li>$J(\vec \theta)$</li>
                <li>$\frac{\delta}{\delta\theta_j}J(\vec \theta)$</li>
            </ul>

            <p>Besides gradient descent, there are several other algorithms that could be used:</p>

            <ul>
                <li>Conjugate Gradient</li>
                <li>BFGS</li>
                <li>L-BFGS</li>
            </ul>

            <div class="two-col">
                <h5>The advantages:</h5>
                <ul>
                    <li>No need to manually pick $\alpha$</li>
                    <li>Often faster than gradient descent</li>
                </ul>

                <h5>The disadvantages:</h5>
                <ul>
                    <li>More complex</li>
                </ul>
            </div>

            <h5>Example for $\text{min}_\theta$</h5>
            $$\vec\theta=\left[\begin{array}{c} \theta_1 \\ \theta_2 \end{array}\right]\\
            J(\vec\theta) = (\theta_1 -5)^2 + (\theta_2 -5)^2\\
            \frac{\delta}{\delta\theta_1}=2(\theta_1 -5)\\
            \frac{\delta}{\delta\theta_2} = 2(\theta_2 - 5)$$

<pre><code class="scilab">function[jVal, gradient] = costFunction(theta)

% code to compute J(theta)
jVal = (theta(1) - 5)^2 + (theta(2)-5)^2;

gradient = zeros(2,1);

% code to compute delta/delta theta_j J(theta)
gradient(1) = 2*(theta(1)-5);
gradient(2) = 2*theta(2)-5);

options = optimset('GradObj', 'on', 'MaxIter', '100');
initialTheta = zeros(2,1);
[optTheta, functionval, exitFlag] = fminunc(@costFunction, initialTheta, options);</code></pre>


            <h4 id="one-vs-all-class">Multiclass Classification: One-vs-all<a class="anchor-link" href="#one-vs-all-class"></a></h4>
            <p>Classification with more than two $y$'s works like this:</p>
            $$h_\theta^{(i)}(x) = P(y=i|x;\theta) \;\;\; (i = 1,2,3,...)$$
            <p>Each $i$ has it's own hypothesis $h_\theta^{(i)}(x)$ and predicts the probability that $y=i$ for each class $i$. On a new input $x$, to make a prediction, pick the class $i$ that maximizes $\text{max}_i h_\theta^{(i)}(x)$</p>


            <h3 id="regularization">Regularization<a class="anchor-link" href="#regularization"></a></h3>
            <p>To prevent <em>overfitting</em> or <em>underfitting</em> of our hypothesis, we can regularize regressions.</p>

            <h4 id="reg-lin-reg">Regularized Linear Regression<a class="anchor-link" href="#reg-lin-reg"></a></h4>
            $$J(\vec\theta) = \frac{1}{2m} \left[\sum^m_{i=1}(h_\theta(x^{(i)}) -y^{(i)})^2 + \lambda \sum^m_{j=1}\theta_j \right]$$

            <p>Where the term prepended by $y$ is called the <em>regularization term</em>. The goal is again to $\text{min}_\theta \; J(\vec\theta)$. The regularized gradient descent looks like this:</p>

            $$\begin{aligned}
            \text{repeat} & \{\\
            & \theta_0 := \theta_0 - \alpha \frac{1}{m} \sum^m_{i=0} (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)}\\
            & \theta_j := \theta_j - \alpha \left[ \frac{1}{m} \sum^m_{i=0} (h_\theta(x^{(i)}) - y^{(i)})x^{(i)}_j + \frac{\lambda}{m} \theta_j \right] \;\; \text{(for j=1...n)}\\
            \} \phantom{15pt} &
            \end{aligned}$$

            <p>Note that $\theta_0$ does not get penalized for over- or underfitting and therefore not regularized. The second term simply is $\frac{\delta}{\delta \theta_j} J(\vec\theta)$. For $j > 0$ the term can also be written as:</p>

            $$\theta_j = \theta_j(1-\alpha \frac{\lambda}{m}) - \alpha \frac{1}{m} \sum^m_{i=0}(h_\theta(x^{(i)}) - y^{(i)})x^{(i)}_j$$

            <p>You can see that written this way, the term is pretty much the same as the original term.</p>

            <h4 id="reg-norm-eq">Regularized Normal Equation<a class="anchor-link" href="#reg-norm-eq"></a></h4>
            $$X = \left[\begin{array}{c} (x^{(1)})^T \\ \vdots \\ (x^{(m)})^T \end{array}\right] \in \mathbb{R}^{mx(n+1)} \;\; \text{with} \;\; y = \left[\begin{array}{c} y^{(1)} \\ \vdots \\ y^{(m)} \end{array}\right] \in \mathbb{R}^{m}$$

            <p>The regularized normal equation now looks like this:</p>

            $$\vec\theta = (X^T X + \lambda \left[\begin{array}{ccccc}
             0 & 0 & 0 & \cdots & 0 \\
             0 & 1 &  0 & \cdots & 0 \\
             0 & 0 & 1 & \cdots & 0 \\
             \vdots & \vdots & \vdots & \ddots & \vdots \\
             0 & 0 & 0 & \cdots & 1 \\
             \end{array}\right])^{-1}X^T y$$

            <p>Where the matrix consisting of zeros and ones is sized $(n+1)x(n+1)$.</p>

            <h4 id="reg-non-inv">Non-Invertibility<a class="anchor-link" href="#reg-non-inv"></a></h4>
            <p>Suppose $m \leq n$ where $m$ is the number of examples and $n$ is the number of features.</p>

            $$\theta = (X^TX)^{-1}X^T y$$

            <p>When $m \leq n$ $X^TX$ is not invertible.</p>
            <p>If $\lambda > 0$ the matrix is invertible because of the regularization.</p>

            <h4 id="reg-log-reg">Regularized Logistic Regression<a class="anchor-link" href="#reg-log-reg"></a></h4>

            $$J(\vec \theta) = - \left[ \frac{1}{m} \sum^{m}_{i=1} log(h_\theta(x^{(i)})) + (1-y^{(i)})log(1- h_\theta(x^{(i)})) \right] + \frac{\lambda}{2m} \sum^m_{i=1} \theta^2_j$$

            <p>To prevent overfitting, $\theta_j$ will be penalized with $y>0$ for being too large.</p>

            <h2 class="week-indicator" id="week-4">Week 4<a class="anchor-link" href="#week-4"></a></h2>
            <h3 id="ann-rep">Neural Networks: Representation<a class="anchor-link" href="#ann-rep"></a></h3>
            <h4 id="ann-hypo">Non-Linear Hypothesis<a class="anchor-link" href="#ann-hypo"></a></h4>
            <p>For more complex hypothesis that are non-linear to fit the hypothesis it might be required to add a number of quadratic or other features. With a higher number of features, this results in a really large hypothesis. This is where neural networks come in.</p>

            <h4 id="ann-mod-rep-1">Model Representation I<a class="anchor-link" href="#ann-mod-rep-1"></a></h4>
            <figure>
                <figcaption>Neurons in the brain</figcaption>
                <img src="img/neurons.png" alt="neurons">
            </figure>

            <ul>
                <li><em>Dendrite:</em> input wires</li>
                <li><em>Axon:</em> output wire</li>
            </ul>

            <figure>
                <figcaption>Neuron Model: Logistic Unit</figcaption>
                <img src="img/neuron_log_model.png" alt="neuron model: logistic unit">
            </figure>

            <ul>
                <li>Hypothesis: $h_\Theta(x) = \frac{1}{1+e^{-\Theta^Tx}}$ (also called <em>sigmoid activation function</em>)</li>
                <li>$x_0$: bias unit with $x_0 = 1$</li>
            </ul>

            <p>The so called <em>weights</em> of the neurons are going to be the parameters of this model.</p>

            <figure>
                <figcaption>A Neural Network</figcaption>
                <img src="img/neural_network.png" alt="neural network">
            </figure>

            <p>Again, $x_0$ and $a_0^(2)$ are the bias units. The first layer ($\{x_0, x_1, x_2, x_3\}$) is called the <em>input layer</em>. The third layer ($\{a^{(2)}_0, a^{(2)}_1, a^{(2)}_2, a^{(2)}_3\}$) is called the output layer. All layers between the input and the output layers are called <em>hidden layers</em>. So in this example, layer 2 is a hidden layer.</p>

            <ul>
                <li>$a^{(j)}_i$: activation of unit $i$ in layer $j$</li>
                <li>$\Theta^{(j)}$: matrix of weights controlling function mapping from layer $j$ to layer $j+1$</li>
            </ul>

            <p>In general, if a network has $s_j$ units in layer $j$, $s_{j+1}$ units in layer $j+1$, then $\Theta^{(j)}$ will be of dimension $s_{j+1} \times (s_j+1)$</p>

            <h4 id="ann-mod-rep-2">Model Representation II<a class="anchor-link" href="#ann-mod-rep-2"></a></h4>
            <h5 id="forward-prop-vec">Forward Propagation: Vectorized Implementation<a class="anchor-link" href="#forward-prop-vec"></a></h5>
            <p>From the first layer to the output layer the values are calculated based on the values of the previous layer.</p>

            $$x=\left[\begin{array}{c} x_0 \\ \vdots \\ x_3 \end{array}\right], \;\; z^{(2)}=\left[\begin{array}{c} z^{(2)}_1 \\ z^{(2)}_2 \\ z^{(2)}_3 \end{array}\right]$$

            <p>With $z^{(2)} = \Theta^{(1)}a^{(1)}$ and $a^{(2)} = g(z^{(3)})$. Basically this is just a logistic regression for each layer $a_i^{(j)}$. For the bias unit we add $a^{(2)}_0 = 1$ ($\Rightarrow a^{(2)} \in \mathbb{R}^{n+1}$). $z^{(3)} = \Theta^{(2)}a^{(2)}$ and $h_\Theta(x) = a^{(3)} = g(z^{(3)})$.</p>

            <figure>
                <figcaption>Other Network Architectures</figcaption>
                <img src="img/net_arch.png" alt="">
            </figure>

            <h4 id="ex-int-1">Examples and Intuitition I<a class="anchor-link" href="#ex-int-1"></a></h4>
            <h5 id="simp-ex-and">Simple Example: AND<a class="anchor-link" href="#ex-int-2"></a></h5>
            <ul>
                <li>$x_1, x_2 \in \{0,1\}$</li>
                <li>$y = x_1 \text{AND} x_2$</li>
            </ul>

            <figure>
                <figcaption>Simple Example: AND</figcaption>
                <img src="img/ann_and.png" alt="">

            </figure>

            <p>We add the following weights to achieve the <em>AND</em> operation:
                <ul>
                    <li>$\Theta_{10}^{(1)} = -30$</li>
                    <li>$\Theta_{11}^{(1)} = 20$</li>
                    <li>$\Theta_{12}^{(1)} = 20$</li>
                </ul>
            </p>

            <br>

            <p>$h_\Theta(x) = g(-30+20x_1+20x_2)$</p>

            <table>
                <thead>
                    <tr>
                        <th>$x_1$</th>
                        <th>$x_2$</th>
                        <th>$h_\Theta(x)$</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>$0$</td>
                        <td>$0$</td>
                        <td>$g(-30) \approx 0$</td>
                    </tr>
                    <tr>
                        <td>$0$</td>
                        <td>$1$</td>
                        <td>$g(-10) \approx 0$</td>
                    </tr>
                    <tr>
                        <td>$1$</td>
                        <td>$0$</td>
                        <td>$g(-10) \approx 0$</td>
                    </tr>
                    <tr>
                        <td>$1$</td>
                        <td>$1$</td>
                        <td>$g(10) \approx 1$</td>
                    </tr>
                </tbody>
            </table>

            <h5 id="simp-ex-or">Simple Example: OR<a class="anchor-link" href="#simp-ex-or"></a></h5>
            <figure>
                <figcaption>Simple Example: OR</figcaption>
                <img src="img/ann_or.png" alt="">
            </figure>

            <p>$h_\Theta(x) = g(-10 + 20x_1 + 20x_2)$</p>

            <table>
                <thead>
                    <tr>
                        <th>$x_1$</th>
                        <th>$x_2$</th>
                        <th>$h_\Theta(x)$</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>$0$</td>
                        <td>$0$</td>
                        <td>$g(-10) \approx 0$</td>
                    </tr>
                    <tr>
                        <td>$0$</td>
                        <td>$1$</td>
                        <td>$g(10) \approx 1$</td>
                    </tr>
                    <tr>
                        <td>$1$</td>
                        <td>$0$</td>
                        <td>$g(10) \approx 1$</td>
                    </tr>
                    <tr>
                        <td>$1$</td>
                        <td>$1$</td>
                        <td>$g(30) \approx 1$</td>
                    </tr>
                </tbody>
            </table>

            <h4 id="ex-int-2">Examples and Intuitition II<a class="anchor-link" href="#ex-int-2"></a></h4>
            <h5 id="simp-ex-not">Negation<a class="anchor-link" href="#simp-ex-not"></a></h5>
            <figure>
                <figcaption>Simple Example: Negation</figcaption>
                <img src="img/ann_not.png" alt="">
            </figure>

            <p>$h_\Theta(x) = g(10 - 20x_1)$</p>

            <table>
                <thead>
                    <tr>
                        <th>$x_1$</th>
                        <th>$h_\Theta(x)$</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>$0$</td>
                        <td>$g(10) \approx 1$</td>
                    </tr>
                    <tr>
                        <td>$1$</td>
                        <td>$g(-10) \approx 0$</td>
                    </tr>
                </tbody>
            </table>

            <h5 id="simp-ex-xnor">XNOR<a class="anchor-link" href="#simp-ex-xnor"></a></h5>
            <figure>
                <figcaption>explanatory caption</figcaption>
                <img src="img/ann_xnor.png" alt="">
            </figure>

            <table>
                <thead>
                    <tr>
                        <th>$x_1$</th>
                        <th>$x_2$</th>
                        <th>$a_1^{(2)}$</th>
                        <th>$a_2^{(2)}$</th>
                        <th>$h_\Theta(x)$</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>$0$</td>
                        <td>$0$</td>
                        <td>$0$</td>
                        <td>$1$</td>
                        <td>$1$</td>
                    </tr>
                    <tr>
                        <td>$0$</td>
                        <td>$1$</td>
                        <td>$0$</td>
                        <td>$0$</td>
                        <td>$0$</td>
                    </tr>
                    <tr>
                        <td>$1$</td>
                        <td>$0$</td>
                        <td>$0$</td>
                        <td>$0$</td>
                        <td>$0$</td>
                    </tr>
                    <tr>
                        <td>$1$</td>
                        <td>$1$</td>
                        <td>$1$</td>
                        <td>$1$</td>
                        <td>$1$</td>
                    </tr>
                </tbody>
            </table>

            <h4 id="ann-multi-class">Multiclass Classification<a class="anchor-link" href="#ann-multi-class"></a></h4>
            <p>$y^{(i)}\in\mathbb{R}^n$ where $n$ is the number of classes.</p>
            $$h_\Theta = \left[\begin{array}{c} 0 \\ \vdots \\ 0 \\ 1 \\ 0 \\ \vdots 0 \end{array}\right]$$
            <p>The $i$-th column is equal to $1$ for the $i$-th feature of the output layer.</p>

            <h4 id="ann-adv-opt">Advanced Optimization</h4>
<pre><code>function[jVal ,gradient] = costFunction(gradient = [code to compute the partial derivitive of theta_j of J(theta)]; theta)</code></pre>

            <p>With $j>1$ (note that octave indexes start at 1).</p>

            <h3>Neural Networks: Learning</h3>
            <h4>Cost Function</h4>
            $$\{ (x^{(1)}, y^{(1)}), …, (x^{(m)}, y^{(m)}) \}\\
            L = \text{number of layers in network}\\
            s_l = \text{numbers of units (that are not bias units)}$$

            <h5>Binary Classification</h5>
            <p>$y \in \{0,1\}$ with one output unit.</p>

            <h5>Multi-Class Classification</h5>
            <p>For $K$ classes: $y \in \mathbb{R}^K$ with $K$ output units.</p>

            <h5>Cost Function</h5>
            <p>The cost function for neural networks is similar to the regularized logistic regressions cost function.</p>
            $$
            h_\Theta(x) \in \mathbb{R}^K \;\; (h_\Theta(x))_i = \text{i-th output}\\
            $$
            $$\begin{align}
            J(\Theta) = &-\frac{1}{m}\left[ \sum^{m}_{i=1} \sum^{K}_{k=1} y^{(i)}_k log(h_\Theta(x^{(i)}))_k + (1-y^{(i)}_k) log(1-(h_\Theta(x{(i)}))_k) \right]\\
                & + \frac{\lambda}{2m} \sum^{L-1}_{l=1}\sum^{s_l}_{i=1}\sum^{s_{l+1}}_{j=1}(\Theta^{(l)}_{ji})^2
            \end{align}
            $$

            <h4>Backpropagation Algorithm</h4>
            <h5>Gradient Computation</h5>
            <p>With $J(\Theta)$ we now need $\text{min}_\Theta$. For that we need to compute:</p>

            <ul>
                <li>$J(\Theta)$</li>
                <li>$\frac{\delta}{\delta \Theta^{(l)}_{ij}} J(\Theta)$ with $\Theta^{(l)}_{ij} \in \mathbb{R}$</li>
            </ul>

            <p>Intuitition: $\delta^{(l)}_j$ is the error of node $j$ in layer $l$. If we for example have a four layer network ($L = 4$) we have to calculate three errors:</p>

            <ul>
                <li>$\delta^{(4)}_j = a^{(4)}_j - y_j$ what basically is $\delta^{(4)} = h_\Theta(x) - y$</li>
                <li>$\delta^{(3)}_j = (\Theta^{(3)})^T \delta^{(4)} .* g'(z^{(3)})$</li>
                <li>$\delta^{(2)}_j = (\Theta^{(2)})^T \delta^{(3)} .* g'(z^{(2)})$</li>
            </ul>

            <h5>Alrogithm: Backpropagation</h5>
            <p>The training set is :$\{ (x^{(1)}, y^{(1)}), ..., (x^{(m)}, y^{(m)}) \}$.
            We now set $\Delta^{(l)}_{ij} = 0 \; \forall \; l,j,i$. $\Delta$ is used to compute $\frac{\delta}{\delta \Theta_{ij}^{(l)}} J(\Theta)$.</p>

            $$\begin{aligned}
                \text{for} \; & i = 1 \; \text{to} \; m\\
                    & \text{set} \; a^{(1)} = x^{(i)}\\
                    & \text{perform forward propagation to compute} \; a^{(l)} \; \text{for} \; l = 2,3,...,L\\
                    & \text{using} \; y^{(i)} \text{, compute} \delta^{(L)} = a^{(L)} - y^{(i)}\\
                    & \text{compute} \; \delta^{(L-1)}, \delta^{(L-2)}, ..., \delta^{(2)}\\
                    & \Delta^{(l)}_{ij} := \Delta^{(l)}_{ij} + a^{(l)}_j \delta^{(l+1)}_i \;\; \text{(vectorized:} \; \Delta^{(l)} = \Delta^{(l)} + \delta^{(l+1)} (a^{(l)})T \; \text{)}\\
                    \\
                    \\
            D^{(l)}_{ij} &= \frac{1}{m} \Delta^{(l)}_{ij} + \lambda\Theta^{(l)}_{ij} \;\; \text{if} \;  \neq 0\\
            D^{(l)}_{ij} &= \frac{1}{m} \Delta^{(l)}_{ij}\\
            \frac{\delta}{\delta \Theta_{ij}^{(l)}} & J(\Theta) = D^{(l)}_{ij}
            \end{aligned}$$

            <h5>Forward Propagation</h5>
            $$\text{cost}(i) = y^{(i)} log(h_\Theta(x^{(i)})) + (1-y^{(i)})log(h_\Theta(x^{(i)}))$$

            <p>The <em>error</em> of $a^{(l)}_{j}$ (unit $j$ in layer $l$) is:</p>
            $$\delta^{(l)}_j = \frac{\delta}{\delta z^{(l)}_j} \text{cost}(i)$$

            <h4>Implementation Note: Unrolling Parameter</h4>
<pre><code class="scilab">function[jval, gradient] = costFunction(theta)
…
optTheta = fminunc(@costFunction, initialTheta, options)</code></pre>

            <p>Where <code>gradient</code> and <code>theta</code> are vectors in $\mathbb{R}^{n+1}$.</p>

            <p>If we have for example a neural network with four layers ($L=4$):</p>
            <ul>
                <li>$\Theta^{(1)}, \Theta^{(2)}, \Theta^{(3)}$ are matrices (<code>Theta1></code>, <code>Theta2</code>, <code>Theta3</code>)</li>
                <li>$D^{(1)}, D^{(2)}, D^{(3)}$ matrices (<code>D1</code>, <code>D2</code>, <code>D3</code>)</li>
            </ul>
            <p>all unrolled into vectors.</p>

            <h5>Example</h5>
            $$s_1 = 10, s_2 = 10, s_3 = 1\\
            \Theta^{(1)} \in \mathbb{R}^{10x11}, \Theta^{(2)} \in \mathbb{R}^{10,11}, \Theta^{(3)} \in \mathbb{R}^{1x11}\\
            D^{(1)} \in \mathbb{R}^{10x11}, D^{(2)} \in \mathbb{R}^{10,11}, D^{(3)} \in \mathbb{R}^{1x11}
            $$

<pre><code class="scilab">thetaVec = [Theta1(:), Theta2(:), Theta3(:)];
DVec = [D1(:), D2(:), D3(:)];

Theta1 = reshape(thetaVec(1:110), 10, 11)
Theta2 = reshape(thetaVec(111:220), 10, 11)
Theta3 = reshape(thetaVec(221:231), 1, 11)
</code></pre>

            <h6>Learning Algorithm</h6>
            <ul>
                <li>Have initial parameters $\Theta^{(1)}, \Theta^{(2)}, \Theta^{(3)}$</li>
                <li>Unroll to get <code>initialTheta</code> to pass to
                <code>fminunc(@costFunction, initialTheta, options)</code></li>
            </ul>

            <p><code>function[jVal, gradientVec] = costFunction(thetaVec)</code></p>
            <ul>
                <li>From <code>thetaVec</code>, get $\Theta^{(1)}, \Theta^{(2)}, \Theta^{(3)}$ via reshape</li>
                <li>Use forward/backward propagation to compute $D^{(1)}, D^{(2)}, D^{(3)}$, unroll to get <code>gradientVec</code></li>
            </ul>

            <h4>Gradient Checking</h4>
            <figure>
                <img src="img/grad-check.png" alt="">
            </figure>

            $$\frac{d}{dJ(\theta)} = \frac{J(\theta + \epsilon)- J(\theta - \epsilon)}{2\epsilon}$$

            <p>with $\epsilon = 10^{-4}$. The implementation would look like this:</p>
<pre><code class="scilab">gradApprox = (J(theta + EPSILON) - (J(theta - EPSION)) / (2 * EPSILON)</code></pre>

            <p>The parameter vector $\theta \mathbb{R}^n$ is the unrolled version of all the $\Theta^{(l)}$. The partial derivitives of the cost function would look like this:</p>

            $$\frac{\delta}{\delta \theta_1} J(\theta) = \frac{J(\theta_1 + \epsilon, \theta_2, ..., \theta_n) - J(\theta_1 - \epsilon, \theta_2, ..., \theta_n)}{2 \epsilon}\\
            \frac{\delta}{\delta \theta_2} J(\theta) = \frac{J(\theta_1, \theta_2 + \epsilon, ..., \theta_n) - J(\theta_1, \theta_2 - \epsilon, ..., \theta_n)}{2 \epsilon}\\
            ... \\
            \frac{\delta}{\delta \theta_n} J(\theta) = \frac{J(\theta_1, \theta_2, ..., \theta_n) + \epsilon - J(\theta_1, \theta_2, ..., \theta_n - \epsilon)}{2 \epsilon}\\
            $$
<pre><code class="scilab">for i = 1:n,
    thetaPlus = theta;
    thetaPlus(i) = thetaPlus(i) + EPSILON;
    thetaMinus = theta;
    thetaPlus(i) = thetaPlus(i) - EPSILON;
    gradApprox(i) = (J(thetaPlus) - J(thetaMinus)) / (2*EPSILON);
</code></pre>

            <p>We then need to check that <code>gradApprox</code> $\approx$ <code>DVec</code> returned from the back propagation.</p>

            <h5>Implementation Note</h5>
            <ul>
                <li>Implement the back propagation to compute <code>DVec</code> (unrolled $D^{(1)}, D^{(2)}, D^{(3)}$)</li>
                <li>Implement numerical gradient checking to compute <code>gradApprox</code></li>
                <li>Make sure they have similar values</li>
                <li>Turn off gradient checking. Using backprop code for learning.</li>
            </ul>

            <p><b>Important:</b> Be sure to disable gradient checking before training your classifier. If you run numerical gradient computation on every iteration of gradient descent your code will be very slow.</p>

            <h4>Random Initialization</h4>
            <p>For gradient descent and advanced optimization method, we need an initial value for $\Theta$.</p>
<pre><code class="scilab">optTheta = fminunc(@costFunction, initialTheta, options)</code></pre>

            <p>Consider gradient descent with <code>initialTheta = zeros(n,1)</code>.</p>

            <h5>Zero Initialization</h5>
            <p>When initialized with zero, after every update the parameters corresponding to the inputs going into each of the hidden units are identical.</p>

            <h5>Random Initialization: Symmetry Breaking</h5>
            <p>Initialize each $\Theta^{(l)}_{ij}$ to random value in $[-\epsilon, \epsilon]$. For example:</p>
<pre><code class="scilab">Theta1 = rand(10,11)*(2*INIT_EPSILON) - INIT_EPSILON;
Theta2 = rand(1, 11)*(2*INIT_EPSILON) - INIT_EPSILON;</code></pre>

            <h4>Putting it together</h4>
            <ul>
                <li>Pick a network architecture (connectivity pattern between neurons)</li>
                <li>Number of inputs: dimensions of features $x^{(i)}$</li>
                <li>Number of output units: number of classes</li>
            </ul>

            <p>(Reasonable default: one hidden layer, or if more than one hidden layer, you need to have the same number of hidden units in every hidden layer (usually the more the better))</p>

            <h5>Training a Neural Network</h5>
            <ul>
                <li>Randomly initialize weights</li>
                <li>Implement forward propagation to get $h_\Theta(x^{(i)})$ for any $x^{(i)}$</li>
                <li>Implement code to compute cost function $J(\Theta)$</li>
                <li>Implement back propagation to compute partial derivitives $\frac{\delta}{\delta \Theta^{(l)}_{jk}} J(\Theta)$
                    <p>For all features:</p>
                    <ul>
                        <li>Perform forward and backward propagation using the example $(x^{(i)}, y^{(i)})$</li>
                        <li>(Get activations $a^{(l)}$ and delta terms $\delta^{(l)}$ for $l=2,…,L$)</li>
                        <li>Compute partial derivitive of the cost function</li>
                    </ul>
                </li>
                <li>Use gradient checking to compare $\frac{\delta}{\delta \Theta^{(l)}_{ij}} J(\Theta)$ computed using backpropagation vs. using numerical estimate of gradient of $J(\Theta)$. Turn off code for gradient checking.</li>
                <li>Use <u>gradient descent</u> or <u>adavanced optimization method</u> with backpropagation to try to minimize $J(\Theta)$ as a function of parameters $\Theta$</li>
            </ul>

            <h2 class="week-indicator" id="week-5">Week 5<a class="anchor-link" href="#week-5"></a></h2>
            <h3>Deciding What to Try Next</h3>
            <h4>Debugging a learning algorithm</h4>
            <p>Suppose you implemented regularized linear regression. However, when you test your hypothesis on a new set of data, you find that it mameks unacceptably large errors in its predictions. What should you try next?</p>

            <ul>
                <li>Get more training examples</li>
                <li>Try a smaller set of features</li>
                <li>Try getting additional features</li>
                <li>Try adding polynomial features</li>
                <li>Try decreasing $\lambda$</li>
                <li>Try increasing $\lambda$</li>
            </ul>

            <h4>Machine Learning Diagnostics</h4>
            <p>Diagnostic: A test that you can run to gain insight what is/isn't working with a learning algorithm, and gain guidance as to how best to improve its performance.</p>
            <p>Diagnostics can take itme to implement, but doing so can be very good use of your time.</p>
            <hr>

            <h3>Evaluating a Hypothesis</h3>
            <p>To test the hypothesis, the training data gets split into a training set (bigger part) and a test set (smaller part). You could for example split them into 70%/30%. Note that the data is sorted, the picks should be randomized, or the data should be randomly shuffled.</p>

            <ul>
                <li>Training set:
                $$\begin{array}{c}
                (x^{(1)},y^{(1)})\\
                (x^{(2)},y^{(2)})\\
                \vdots \\
                (x^{(m)},y^{(m)})
                \end{array}$$</li>
                <li>Test set (with $m_\text{test}$: number of examples):
                $$\begin{array}{c}
                (x^{(1)}_\text{test},y^{(1)}_\text{test})\\
                (x^{(2)}_\text{test},y^{(2)}_\text{test})\\
                \vdots \\
                (x^{(m)}_\text{test},y^{(m)}_\text{test})
                \end{array}$$</li>
            </ul>

            <h4>Training/Testing Procedure for Linear Regression</h4>
            <p>Learn parameter $\theta$ from training data (minimizing training error $J(\theta)$ and compute the test set error:</p>
            $$ J_\text{test}(\theta) =\frac{1}{2m_\text{test}} \sum^{m_\text{test}}_{i=1} \left( h_\theta(x^{(i)}_\text{test} - y^{(i)}_\text{test}) \right) $$

            <p>The misclassification error:</p>

            $$\text{err}(h_\theta(x), y) = \left\{\begin{array}{l}
                \text{if} \; h_\theta(x) \geq 0.5 \;\; y=0 \\
                \text{or if} \; h_\theta(x) \lt 0.5 \;\; y=1
            \end{array}\right.$$

            <p>The test error is:</p>
             $$\frac{1}{m_\text{test}} \sum^{m_\text{test}}_{i=1} \text{err}(h_\theta(x^{(i)}_\text{test}), y^{(i)}_\text{test})$$

            <h4>Model Selection and Train/Validation/Test Sets</h4>
            <p>Once parameters $\theta_0, \theta_1, \cdots, \theta_4$ were fit to some data (training set), the error of the parameters as measured on that data (the training error $J(\vec\theta)$) is likely to be lower than the actual generalization error.</p>

            <h5>Model Selection</h5>
            <ol>
                <li>$h_\theta(x) = \theta_0 + \theta_1 x$ with $d=1$ $\rightarrow$ $\Theta^{(1)} \rightarrow J_\text{test}(\Theta^{(1)})$</li>
                <li>$h_\theta(x) = \theta_0 + \theta_1 x + \theta_2 x^2$ with $d=2$ $\rightarrow$ $\Theta^{(2)} \rightarrow J_\text{test}(\Theta^{(2)})$</li>
                <li>$h_\theta(x) = \theta_0 + \theta_1 x + \theta_2 x^2 + \theta_3 x^3$ with $d=3$  $\rightarrow$ $\Theta^{(3)} \rightarrow J_\text{test}(\Theta^{(3)})$</li>
                <li>…</li>
                <li>$h_\theta(x) = \theta_0 + \theta_1 x + \cdots + \theta_10 x^10$ with $d=10$  $\rightarrow$ $\Theta^{(10)} \rightarrow J_\text{test}(\Theta^{(10)})$</li>
            </ol>

            <p>In order to select a model you could take the hypothesis with the lowest <em>test set error</em>. (Let's say for now it's $\theta_0 + \cdots + \theta_5x^5$). Now how well does it generalize? report the test set error $J_\text{test}(\theta^{(5)}$</p>
            <p>The problem is, that $J_\text{test}(\theta^{(5)}$ is likely to be an optimistic estimate of generalization error, i.e. our extra parameter ($d$ = degree of polynomial) is fit to the test set.</p>

            <p>Now instead of splitting it 70/30 we split the dataset like this: 60% training set, 20% cross validation set ($cv$) and 20% test set.</p>

            <ul>
                <li>$$\begin{array}{c}
                (x^{(1)}, y^{(1)})\\
                (x^{(2)}, y^{(2)})\\
                \vdots\\
                (x^{(m)}, y^{(m)})\\
                \end{array}$$</li>
                <li>$$\begin{array}{c}
                (x^{(1)}_{cv}, y^{(1)}_{cv})\\
                (x^{(2)}_{cv}, y^{(2)}_{cv})\\
                \vdots\\
                (x^{(m)}_{cv}, y^{(m)}_{cv})\\
                \end{array}$$</li>
                <li>$$\begin{array}{c}
                (x^{(1)}_{\text{test}}, y^{(1)}_{\text{test}})\\
                (x^{(2)}_{\text{test}}, y^{(2)}_{\text{test}})\\
                \vdots\\
                (x^{(m)}_{\text{test}}, y^{(m)}_{\text{test}})\\
                \end{array}$$</li>
            </ul>

            <p>Where $m_{cv}$ is the number of cross validation examples.</p>

            <ul>
                <li>Training error: $J(\theta) = \frac{1}{2m} \sum^{m}_{i=1} (h_\theta(x^{(i)})-y^{(i)})^2$</li>
                <li>Cross Validation error: $J_{cv}(\theta) = \frac{1}{2m_{cv}} \sum^{m_cv}_{i=1} (h_\theta(x^{(i)}_{cv})-y^{(i)}_{cv})^2$</li>
                <li>Cross Validation error: $J_{\text{test}}(\theta) = \frac{1}{2m_\text{test}} \sum^{m_\text{test}}_{i=1} (h_\theta(x^{(i)}_{\text{test}})-y^{(i)}_{\text{test}})^2$</li>
            </ul>

            <p>Now to select a model you calculate $min_\theta J(\theta)$ for $\theta^{(i)}$ and then calculate $J_{cv}(\theta)$. You then select the model with the lowest cross validation error.</p>

            <h4>Diagnosing Bias vs. Variance</h4>
            <figure>
                <figcaption>Bias vs. Variance</figcaption>
                <img src="img/bias_vs_variance.png" alt="">
            </figure>

            <p>Suppose your learning algorithm is performing less wellt han yoh were hoping. ($J_{cv}(\theta)$ or $J_\text{test}(\theta)$ is high.) Is it as bias problem or a variance problem?</p>

            <figure>
                <figcaption>Train vs. Cross Validation Error</figcaption>
                <img src="img/train_vs_cross.png" alt="">
            </figure>

            <ul>
                <li><b>Bias (underfit):</b> $J_\text{train}(\theta)$ will be high, $J_{cv}(\theta) \approx J_\text{train}(\theta)$</li>
                <li><b>Variance (overfit):</b> $J_\text{train}(\theta)$ will be low, $J_{cv}(\theta) \gt\gt J_\text{train}(\theta)$</li>
            </ul>

            <figure>
                <figcaption>Bias or Variance?</figcaption>
                <img src="img/bias_or_variance.png" alt="">
            </figure>

            <h4>Regularization and Bias/Variance</h4>
            <h5>Linear Regression with Regularization</h5>
            <h5>Choosing the Regularization Parameter $\lambda$</h5>
            <h5>Bias/Variance as a Function of the Regularization Parameter $\lambda$</h5>
            <h4>Learning Curves</h4>


            <h2 id="appendix">Appendix<a class="anchor-link" href="#appendix"></a></h2>
            <h3 id="footnotes">Footnotes<a class="anchor-link" href="#footnotes"></a></h3>

            <ol>
                <li id="footnote1">Taken from the Machine Learning Video Lecture: »What is Machine Learning?«, Minute <code>01:49</code> at <a href="https://class.coursera.org/ml-008/lecture/2" title="Video Lecture on Coursera">Coursera</a></li>
            </ol>

            <footer class="main-foot">
                <ul>
                    <li><a href="#top">Back to top</a></li>
                    <li><a href="http://www.kopimi.com/kopimi" title="kopimi"><img src="img/kopimi.svg" class="kopimi"></a></li>
                    <li>By Christian Schulze<br><a href="http://andinfinity.de">andinfinity.de</a></li>
                </ul>
            </footer>
        </div>
    </body>
</html>
